"""
ThreadGram Scraper - ç”¨ Playwright çˆ¬å– Threads åœ–ç‰‡
"""
import asyncio
import json
import re
import sys
from pathlib import Path
from datetime import datetime

try:
    from playwright.async_api import async_playwright
except ImportError:
    print("è«‹å…ˆå®‰è£ playwright: pip install playwright && playwright install chromium")
    sys.exit(1)


async def scrape_threads(username: str, max_scrolls: int = 30):
    """çˆ¬å–æŒ‡å®šç”¨æˆ¶çš„ Threads åœ–ç‰‡"""

    print(f"ðŸš€ é–‹å§‹çˆ¬å– @{username} çš„è²¼æ–‡...")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        url = f"https://www.threads.net/@{username}"
        print(f"ðŸ“ å‰å¾€ {url}")

        await page.goto(url, wait_until="networkidle")
        await asyncio.sleep(3)  # ç­‰å¾…åˆå§‹è¼‰å…¥

        # æ”¶é›†æ‰€æœ‰åœ–ç‰‡ URL
        all_images = set()
        last_count = 0
        no_new_count = 0

        for i in range(max_scrolls):
            # æŠ“å–ç›®å‰é é¢çš„åœ–ç‰‡
            images = await page.evaluate('''() => {
                const imgs = document.querySelectorAll('img');
                return Array.from(imgs).map(img => img.src).filter(src =>
                    src.includes('cdninstagram.com') &&
                    !src.includes('s150x150') &&
                    !src.includes('s64x64') &&
                    !src.includes('s32x32')
                );
            }''')

            for img in images:
                all_images.add(img)

            current_count = len(all_images)
            print(f"ðŸ“¸ æ»¾å‹• {i+1}/{max_scrolls} - å·²æ”¶é›† {current_count} å¼µåœ–ç‰‡", end='\r')

            # æª¢æŸ¥æ˜¯å¦æœ‰æ–°åœ–ç‰‡
            if current_count == last_count:
                no_new_count += 1
                if no_new_count >= 5:
                    print(f"\nâœ… é€£çºŒ 5 æ¬¡æ²’æœ‰æ–°åœ–ç‰‡ï¼Œåœæ­¢æ»¾å‹•")
                    break
            else:
                no_new_count = 0

            last_count = current_count

            # æ»¾å‹•
            await page.evaluate('window.scrollBy(0, 1000)')
            await asyncio.sleep(1.5)

        await browser.close()

    print(f"\nðŸŽ‰ å…±æ”¶é›† {len(all_images)} å¼µåœ–ç‰‡")

    # åˆ†çµ„åœ–ç‰‡ï¼ˆæ ¹æ“š URL ä¸­çš„æ•¸å­—æ¨¡å¼ï¼‰
    posts = group_images(list(all_images))

    # å„²å­˜çµæžœ
    output_dir = Path("data")
    output_dir.mkdir(exist_ok=True)

    output_file = output_dir / f"{username}.json"

    result = {
        "username": username,
        "scraped_at": datetime.now().isoformat(),
        "total_images": len(all_images),
        "total_posts": len(posts),
        "posts": posts
    }

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"ðŸ’¾ å·²å„²å­˜è‡³ {output_file}")
    print(f"ðŸ“Š å…± {len(posts)} ç¯‡è²¼æ–‡")

    return result


def group_images(images: list) -> list:
    """å°‡åœ–ç‰‡æŒ‰è²¼æ–‡åˆ†çµ„"""

    # æŒ‰ URL ä¸­çš„æ•¸å­— ID åˆ†çµ„
    groups = {}

    for url in images:
        # æå–åœ–ç‰‡ ID (ç¬¬ä¸€çµ„æ•¸å­—_ç¬¬äºŒçµ„æ•¸å­—å‰9ä½)
        match = re.search(r'/(\d+)_(\d{9})', url)
        if match:
            post_id = match.group(2)  # ç”¨ç¬¬äºŒçµ„æ•¸å­—å‰9ä½ä½œç‚ºè²¼æ–‡ID
            if post_id not in groups:
                groups[post_id] = []
            groups[post_id].append(url)
        else:
            # ç„¡æ³•è­˜åˆ¥çš„åœ–ç‰‡ç¨ç«‹æˆçµ„
            groups[f"single_{len(groups)}"] = [url]

    # è½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼ï¼Œæ¯å¼µåœ–ç‰‡åŽ»é‡ï¼ˆä¸åŒå°ºå¯¸ï¼‰
    posts = []
    for post_id, urls in groups.items():
        # åŽ»é‡åŒä¸€å¼µåœ–çš„ä¸åŒå°ºå¯¸
        unique_images = dedupe_sizes(urls)
        if unique_images:
            posts.append(unique_images)

    return posts


def dedupe_sizes(urls: list) -> list:
    """åŽ»é™¤åŒä¸€å¼µåœ–çš„ä¸åŒå°ºå¯¸ç‰ˆæœ¬"""

    seen_ids = {}

    for url in urls:
        # æå–å®Œæ•´åœ–ç‰‡ ID
        match = re.search(r'/(\d+_\d+_\d+_n)', url)
        if match:
            img_id = match.group(1)
            # åˆ¤æ–·å„ªå…ˆç´šï¼ˆæ²’æœ‰å°ºå¯¸æ¨™è¨˜çš„å„ªå…ˆï¼‰
            priority = 0
            if 's150x150' in url: priority = 1
            elif 's320x320' in url: priority = 2
            elif 's640x640' in url: priority = 3
            else: priority = 4

            if img_id not in seen_ids or priority > seen_ids[img_id][1]:
                seen_ids[img_id] = (url, priority)
        else:
            seen_ids[url] = (url, 0)

    return [url for url, _ in seen_ids.values()]


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python scraper.py <username> [max_scrolls]")
        print("ç¯„ä¾‹: python scraper.py boooooook__ 50")
        sys.exit(1)

    username = sys.argv[1].replace("@", "")
    max_scrolls = int(sys.argv[2]) if len(sys.argv) > 2 else 30

    asyncio.run(scrape_threads(username, max_scrolls))
